{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "title_cell",
   "metadata": {},
   "source": [
    "# üöó Auto Insurance Claim Frequency Modeling\n",
    "\n",
    "## Comparative Analysis of GLM and Machine Learning Approaches\n",
    "\n",
    "---\n",
    "\n",
    "**Author:** [Your Name]  \n",
    "**Date:** December 2024  \n",
    "**Institution:** ESILV - √âcole Sup√©rieure d'Ing√©nieurs L√©onard de Vinci  \n",
    "**Program:** Master 1 - Actuarial Science\n",
    "\n",
    "---\n",
    "\n",
    "### üìã Executive Summary\n",
    "\n",
    "This project implements a comprehensive comparison of machine learning models for predicting claim frequency in auto insurance. Using the French Motor Third-Party Liability (freMTPL2freq) dataset containing **678,013 insurance policies**, we evaluate six different modeling approaches.\n",
    "\n",
    "### üéØ Objectives\n",
    "\n",
    "1. Compare traditional actuarial models (Poisson GLM) with machine learning approaches\n",
    "2. Identify the most predictive features for claim frequency\n",
    "3. Evaluate model performance using appropriate metrics for count data\n",
    "4. Provide actionable insights for insurance pricing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "toc",
   "metadata": {},
   "source": [
    "---\n",
    "## üìë Table of Contents\n",
    "\n",
    "1. [Data Loading and Libraries](#1)\n",
    "2. [Exploratory Data Analysis](#2)\n",
    "3. [Data Preprocessing](#3)\n",
    "4. [Feature Engineering](#4)\n",
    "5. [Model Training](#5)\n",
    "6. [Model Comparison](#6)\n",
    "7. [Feature Importance](#7)\n",
    "8. [Hyperparameter Optimization](#8)\n",
    "9. [Model Calibration](#9)\n",
    "10. [Resampling Analysis](#10)\n",
    "11. [Conclusion](#11)\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section1",
   "metadata": {},
   "source": [
    "<a id='1'></a>\n",
    "## 1. Data Loading and Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# IMPORTS AND CONFIGURATION\n",
    "# ============================================\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Set visualization style\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "sns.set_palette(\"husl\")\n",
    "plt.rcParams['figure.figsize'] = [12, 6]\n",
    "plt.rcParams['font.size'] = 11\n",
    "plt.rcParams['axes.titlesize'] = 14\n",
    "plt.rcParams['axes.labelsize'] = 12\n",
    "\n",
    "# Preprocessing and Models\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.svm import LinearSVR\n",
    "from sklearn.linear_model import LinearRegression, PoissonRegressor\n",
    "\n",
    "# Metrics\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_poisson_deviance, mean_absolute_error\n",
    "\n",
    "print(\"‚úÖ All libraries imported successfully!\")\n",
    "print(f\"üì¶ pandas: {pd.__version__}, numpy: {np.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "load_data",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "df = pd.read_csv(\"freMTPL2freq.csv\")\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"üìä DATASET OVERVIEW\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nüìÅ Shape: {df.shape[0]:,} rows √ó {df.shape[1]} columns\")\n",
    "print(f\"üíæ Memory: {df.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "variables_desc",
   "metadata": {},
   "source": [
    "### 1.1 Variables Description\n",
    "\n",
    "| Variable | Type | Description | Actuarial Relevance |\n",
    "|----------|------|-------------|---------------------|\n",
    "| `IDpol` | ID | Policy identifier | Not predictive |\n",
    "| `ClaimNb` | Target | Number of claims | **Target variable** |\n",
    "| `Exposure` | Numeric | Duration in years (0-1) | Offset in GLM |\n",
    "| `Area` | Categorical | Geographic area (A-F) | Risk segmentation |\n",
    "| `VehPower` | Numeric | Vehicle power | Vehicle risk |\n",
    "| `VehAge` | Numeric | Vehicle age (years) | Vehicle risk |\n",
    "| `DrivAge` | Numeric | Driver's age | Driver risk |\n",
    "| `BonusMalus` | Numeric | Bonus-malus coefficient | Claims history |\n",
    "| `VehBrand` | Categorical | Vehicle brand | Vehicle risk |\n",
    "| `VehGas` | Categorical | Fuel type | Vehicle characteristics |\n",
    "| `Density` | Numeric | Population density | Geographic risk |\n",
    "| `Region` | Categorical | French region | Geographic risk |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "first_look",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section2",
   "metadata": {},
   "source": [
    "<a id='2'></a>\n",
    "## 2. Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eda_target",
   "metadata": {},
   "source": [
    "### 2.1 Target Variable Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "target_stats",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create frequency variable\n",
    "df['Frequency'] = df['ClaimNb'] / df['Exposure']\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"üéØ TARGET VARIABLE: ClaimNb\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Distribution\n",
    "claim_dist = df['ClaimNb'].value_counts().sort_index()\n",
    "print(f\"\\nüìä Distribution:\")\n",
    "for claims, count in claim_dist.items():\n",
    "    pct = count / len(df) * 100\n",
    "    print(f\"   {claims} claims: {count:,} ({pct:.2f}%)\")\n",
    "\n",
    "zero_pct = (df['ClaimNb'] == 0).sum() / len(df) * 100\n",
    "print(f\"\\n‚ö†Ô∏è Zero-inflation: {zero_pct:.2f}% - Highly imbalanced data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "target_viz",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Target visualization\n",
    "fig, axes = plt.subplots(1, 3, figsize=(16, 5))\n",
    "\n",
    "# Plot 1: ClaimNb Distribution\n",
    "colors = ['#2ecc71' if x == 0 else '#e74c3c' for x in range(6)]\n",
    "claim_counts = df['ClaimNb'].value_counts().sort_index()\n",
    "bars = axes[0].bar(claim_counts.index, claim_counts.values, color=colors[:len(claim_counts)], \n",
    "                   edgecolor='black', alpha=0.8)\n",
    "axes[0].set_xlabel('Number of Claims')\n",
    "axes[0].set_ylabel('Number of Policies')\n",
    "axes[0].set_title('üìä Distribution of Claims per Policy', fontweight='bold')\n",
    "\n",
    "# Add percentage labels\n",
    "for i, (idx, val) in enumerate(claim_counts.items()):\n",
    "    pct = val / len(df) * 100\n",
    "    axes[0].annotate(f'{pct:.1f}%', xy=(idx, val), ha='center', va='bottom', fontsize=10, fontweight='bold')\n",
    "\n",
    "# Plot 2: Exposure Distribution\n",
    "axes[1].hist(df['Exposure'], bins=50, color='#3498db', edgecolor='black', alpha=0.7)\n",
    "axes[1].axvline(df['Exposure'].mean(), color='red', linestyle='--', linewidth=2, \n",
    "                label=f'Mean: {df[\"Exposure\"].mean():.2f}')\n",
    "axes[1].set_xlabel('Exposure (years)')\n",
    "axes[1].set_ylabel('Frequency')\n",
    "axes[1].set_title('üìä Distribution of Exposure', fontweight='bold')\n",
    "axes[1].legend()\n",
    "\n",
    "# Plot 3: Frequency Distribution (non-zero)\n",
    "freq_nonzero = df[df['Frequency'] > 0]['Frequency']\n",
    "axes[2].hist(freq_nonzero, bins=50, color='#9b59b6', edgecolor='black', alpha=0.7)\n",
    "axes[2].set_xlabel('Claim Frequency (ClaimNb / Exposure)')\n",
    "axes[2].set_ylabel('Count')\n",
    "axes[2].set_title('üìä Claim Frequency (Non-Zero Only)', fontweight='bold')\n",
    "axes[2].set_xlim(0, 15)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('fig01_target_distribution.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eda_num",
   "metadata": {},
   "source": [
    "### 2.2 Numerical Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "num_vars",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_cols = ['VehPower', 'VehAge', 'DrivAge', 'BonusMalus', 'Density', 'Exposure']\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "axes = axes.flatten()\n",
    "colors = ['#3498db', '#e74c3c', '#2ecc71', '#9b59b6', '#f39c12', '#1abc9c']\n",
    "\n",
    "for i, col in enumerate(num_cols):\n",
    "    axes[i].hist(df[col], bins=50, color=colors[i], edgecolor='black', alpha=0.7)\n",
    "    axes[i].axvline(df[col].mean(), color='red', linestyle='--', linewidth=2)\n",
    "    axes[i].axvline(df[col].median(), color='green', linestyle='-.', linewidth=2)\n",
    "    axes[i].set_xlabel(col)\n",
    "    axes[i].set_ylabel('Frequency')\n",
    "    axes[i].set_title(f'üìä {col}', fontweight='bold')\n",
    "    \n",
    "    # Stats annotation\n",
    "    stats_text = f'Mean: {df[col].mean():.1f}\\nMedian: {df[col].median():.1f}'\n",
    "    axes[i].text(0.95, 0.95, stats_text, transform=axes[i].transAxes, \n",
    "                 verticalalignment='top', horizontalalignment='right',\n",
    "                 bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5), fontsize=9)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('fig02_numerical_distributions.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eda_cat",
   "metadata": {},
   "source": [
    "### 2.3 Categorical Variables - Claim Rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cat_vars",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_cols = ['Area', 'VehBrand', 'VehGas', 'Region']\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, col in enumerate(cat_cols):\n",
    "    claim_rate = df.groupby(col).agg({'ClaimNb': 'sum', 'Exposure': 'sum'}).reset_index()\n",
    "    claim_rate['ClaimRate'] = claim_rate['ClaimNb'] / claim_rate['Exposure']\n",
    "    claim_rate = claim_rate.sort_values('ClaimRate', ascending=True)\n",
    "    \n",
    "    if len(claim_rate) > 10:\n",
    "        claim_rate = claim_rate.tail(10)\n",
    "    \n",
    "    bars = axes[i].barh(claim_rate[col].astype(str), claim_rate['ClaimRate'], \n",
    "                        color=plt.cm.RdYlGn_r(np.linspace(0.2, 0.8, len(claim_rate))),\n",
    "                        edgecolor='black', alpha=0.8)\n",
    "    axes[i].set_xlabel('Claim Rate')\n",
    "    axes[i].set_title(f'üìä Claim Rate by {col}', fontweight='bold')\n",
    "    axes[i].axvline(df['ClaimNb'].sum() / df['Exposure'].sum(), color='red', \n",
    "                    linestyle='--', linewidth=2, label='Portfolio Average')\n",
    "    axes[i].legend(loc='lower right')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('fig03_categorical_claim_rates.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eda_corr",
   "metadata": {},
   "source": [
    "### 2.4 Correlation Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "correlation",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Correlation matrix\n",
    "corr_cols = ['ClaimNb', 'Exposure', 'VehPower', 'VehAge', 'DrivAge', 'BonusMalus', 'Density']\n",
    "corr_matrix = df[corr_cols].corr()\n",
    "\n",
    "mask = np.triu(np.ones_like(corr_matrix, dtype=bool))\n",
    "sns.heatmap(corr_matrix, mask=mask, annot=True, cmap='RdBu_r', center=0,\n",
    "            square=True, linewidths=0.5, ax=axes[0], fmt='.3f')\n",
    "axes[0].set_title('üîó Correlation Matrix', fontweight='bold')\n",
    "\n",
    "# Correlation with target\n",
    "target_corr = corr_matrix['ClaimNb'].drop('ClaimNb').sort_values()\n",
    "colors = ['#e74c3c' if x < 0 else '#2ecc71' for x in target_corr.values]\n",
    "axes[1].barh(target_corr.index, target_corr.values, color=colors, edgecolor='black', alpha=0.8)\n",
    "axes[1].axvline(0, color='black', linewidth=0.8)\n",
    "axes[1].set_xlabel('Correlation Coefficient')\n",
    "axes[1].set_title('üéØ Correlation with Target (ClaimNb)', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('fig04_correlation_analysis.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüí° Linear correlations are weak - expected for rare count data.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eda_bivar",
   "metadata": {},
   "source": [
    "### 2.5 Key Risk Factors Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bivariate",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Driver Age vs Claim Rate\n",
    "df['DrivAge_Group'] = pd.cut(df['DrivAge'], bins=[17, 25, 35, 45, 55, 65, 100], \n",
    "                              labels=['18-25', '26-35', '36-45', '46-55', '56-65', '65+'])\n",
    "age_rate = df.groupby('DrivAge_Group').apply(lambda x: x['ClaimNb'].sum() / x['Exposure'].sum()).reset_index()\n",
    "age_rate.columns = ['DrivAge_Group', 'ClaimRate']\n",
    "\n",
    "bars1 = axes[0].bar(age_rate['DrivAge_Group'], age_rate['ClaimRate'], \n",
    "                    color=plt.cm.Reds(np.linspace(0.3, 0.9, len(age_rate))),\n",
    "                    edgecolor='black', alpha=0.8)\n",
    "avg_rate = df['ClaimNb'].sum() / df['Exposure'].sum()\n",
    "axes[0].axhline(avg_rate, color='blue', linestyle='--', linewidth=2, label='Portfolio Average')\n",
    "axes[0].set_xlabel('Driver Age Group')\n",
    "axes[0].set_ylabel('Claim Rate')\n",
    "axes[0].set_title('üë§ Claim Rate by Driver Age', fontweight='bold')\n",
    "axes[0].legend()\n",
    "\n",
    "# Add deviation labels\n",
    "for bar, rate in zip(bars1, age_rate['ClaimRate']):\n",
    "    diff = (rate - avg_rate) / avg_rate * 100\n",
    "    color = 'red' if diff > 0 else 'green'\n",
    "    axes[0].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.002,\n",
    "                 f'{diff:+.0f}%', ha='center', va='bottom', fontsize=9, color=color, fontweight='bold')\n",
    "\n",
    "# Bonus-Malus vs Claim Rate\n",
    "df['BM_Group'] = pd.cut(df['BonusMalus'], bins=[0, 50, 60, 80, 100, 150, 300], \n",
    "                         labels=['50', '51-60', '61-80', '81-100', '101-150', '150+'])\n",
    "bm_rate = df.groupby('BM_Group').apply(lambda x: x['ClaimNb'].sum() / x['Exposure'].sum()).reset_index()\n",
    "bm_rate.columns = ['BM_Group', 'ClaimRate']\n",
    "\n",
    "axes[1].bar(bm_rate['BM_Group'], bm_rate['ClaimRate'], \n",
    "            color=plt.cm.Oranges(np.linspace(0.3, 0.9, len(bm_rate))),\n",
    "            edgecolor='black', alpha=0.8)\n",
    "axes[1].axhline(avg_rate, color='blue', linestyle='--', linewidth=2, label='Portfolio Average')\n",
    "axes[1].set_xlabel('Bonus-Malus Coefficient')\n",
    "axes[1].set_ylabel('Claim Rate')\n",
    "axes[1].set_title('üìà Claim Rate by Bonus-Malus', fontweight='bold')\n",
    "axes[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('fig05_risk_factors.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüí° Actuarial Insights:\")\n",
    "print(\"   ‚Ä¢ Young drivers (18-25) have significantly higher claim rates\")\n",
    "print(\"   ‚Ä¢ Bonus-Malus strongly correlates with future claims\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section3",
   "metadata": {},
   "source": [
    "<a id='3'></a>\n",
    "## 3. Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "preprocessing",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"üîß DATA PREPROCESSING\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(f\"\\nOriginal: {df.shape[0]:,} rows\")\n",
    "\n",
    "# Filters\n",
    "df = df[df['DrivAge'] >= 18]\n",
    "df = df[df['VehAge'] <= 30]\n",
    "df = df[df['Exposure'] > 0]\n",
    "\n",
    "print(f\"After cleaning: {df.shape[0]:,} rows\")\n",
    "\n",
    "# One-hot encoding\n",
    "cat_cols = ['Area', 'VehBrand', 'VehGas', 'Region']\n",
    "df1 = pd.get_dummies(df, columns=cat_cols, drop_first=True)\n",
    "print(f\"After encoding: {df1.shape[1]} columns\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section4",
   "metadata": {},
   "source": [
    "<a id='4'></a>\n",
    "## 4. Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feature_eng",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"‚öôÔ∏è FEATURE ENGINEERING\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "df1['LogDensity'] = np.log1p(df1['Density'])\n",
    "df1['YoungDriver'] = (df1['DrivAge'] < 26).astype(int)\n",
    "df1['SeniorDriver'] = (df1['DrivAge'] > 65).astype(int)\n",
    "df1['NewVehicle'] = (df1['VehAge'] <= 2).astype(int)\n",
    "df1['OldVehicle'] = (df1['VehAge'] > 10).astype(int)\n",
    "df1['HighRisk'] = (df1['BonusMalus'] > 100).astype(int)\n",
    "\n",
    "print(\"\\n‚úÖ Created: LogDensity, YoungDriver, SeniorDriver, NewVehicle, OldVehicle, HighRisk\")\n",
    "print(f\"Total features: {df1.shape[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section5",
   "metadata": {},
   "source": [
    "<a id='5'></a>\n",
    "## 5. Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "train_test",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data\n",
    "y = df1['ClaimNb']\n",
    "X = df1.drop(columns=['ClaimNb', 'Frequency', 'IDpol', 'DrivAge_Group', 'BM_Group'])\n",
    "exposure = df1['Exposure']\n",
    "\n",
    "X_train, X_test, y_train, y_test, exp_train, exp_test = train_test_split(\n",
    "    X, y, exposure, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "print(f\"Training: {X_train.shape[0]:,} samples\")\n",
    "print(f\"Test: {X_test.shape[0]:,} samples\")\n",
    "print(f\"Features: {X_train.shape[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "train_models",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"ü§ñ MODEL TRAINING\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "models = {}\n",
    "predictions = {}\n",
    "\n",
    "X_train_pois = X_train.drop(columns=['Exposure'])\n",
    "X_test_pois = X_test.drop(columns=['Exposure'])\n",
    "\n",
    "# 1. Linear Regression\n",
    "print(\"\\n[1/6] Linear Regression...\")\n",
    "lin_pipeline = Pipeline([('scaler', StandardScaler()), ('linreg', LinearRegression())])\n",
    "lin_pipeline.fit(X_train, y_train)\n",
    "predictions['Linear Regression'] = lin_pipeline.predict(X_test)\n",
    "models['Linear Regression'] = lin_pipeline\n",
    "\n",
    "# 2. Decision Tree\n",
    "print(\"[2/6] Decision Tree...\")\n",
    "tree = DecisionTreeRegressor(max_depth=10, min_samples_leaf=100, random_state=42)\n",
    "tree.fit(X_train, y_train)\n",
    "predictions['Decision Tree'] = tree.predict(X_test)\n",
    "models['Decision Tree'] = tree\n",
    "\n",
    "# 3. Random Forest\n",
    "print(\"[3/6] Random Forest...\")\n",
    "rf = RandomForestRegressor(n_estimators=100, max_depth=15, min_samples_leaf=50, random_state=42, n_jobs=-1)\n",
    "rf.fit(X_train, y_train)\n",
    "predictions['Random Forest'] = rf.predict(X_test)\n",
    "models['Random Forest'] = rf\n",
    "\n",
    "# 4. Gradient Boosting\n",
    "print(\"[4/6] Gradient Boosting...\")\n",
    "gb = GradientBoostingRegressor(n_estimators=100, max_depth=5, learning_rate=0.1, random_state=42)\n",
    "gb.fit(X_train, y_train)\n",
    "predictions['Gradient Boosting'] = gb.predict(X_test)\n",
    "models['Gradient Boosting'] = gb\n",
    "\n",
    "# 5. Linear SVR\n",
    "print(\"[5/6] Linear SVR...\")\n",
    "svr_pipeline = Pipeline([('scaler', StandardScaler()), ('svr', LinearSVR(random_state=42, dual='auto', max_iter=5000))])\n",
    "svr_pipeline.fit(X_train, y_train)\n",
    "predictions['Linear SVR'] = svr_pipeline.predict(X_test)\n",
    "models['Linear SVR'] = svr_pipeline\n",
    "\n",
    "# 6. Poisson Regression\n",
    "print(\"[6/6] Poisson Regression...\")\n",
    "pois = PoissonRegressor(alpha=1e-4, max_iter=1000)\n",
    "pois.fit(X_train_pois, y_train, sample_weight=exp_train)\n",
    "predictions['Poisson Regression'] = pois.predict(X_test_pois)\n",
    "models['Poisson Regression'] = pois\n",
    "\n",
    "print(\"\\n‚úÖ ALL MODELS TRAINED!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section6",
   "metadata": {},
   "source": [
    "<a id='6'></a>\n",
    "## 6. Model Comparison and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "evaluation",
   "metadata": {},
   "outputs": [],
   "source": [
    "def safe_pred(y_pred):\n",
    "    return np.maximum(y_pred, 1e-9)\n",
    "\n",
    "# Calculate metrics\n",
    "results = []\n",
    "for name, y_pred in predictions.items():\n",
    "    y_pred_safe = safe_pred(y_pred)\n",
    "    results.append({\n",
    "        'Model': name,\n",
    "        'MSE': mean_squared_error(y_test, y_pred),\n",
    "        'RMSE': np.sqrt(mean_squared_error(y_test, y_pred)),\n",
    "        'MAE': mean_absolute_error(y_test, y_pred),\n",
    "        'Poisson Deviance': mean_poisson_deviance(y_test, y_pred_safe),\n",
    "        'R¬≤': r2_score(y_test, y_pred)\n",
    "    })\n",
    "\n",
    "results_df = pd.DataFrame(results).sort_values('Poisson Deviance').reset_index(drop=True)\n",
    "results_df['Rank'] = range(1, len(results_df) + 1)\n",
    "\n",
    "print(\"\\nüìä MODEL PERFORMANCE (sorted by Poisson Deviance):\")\n",
    "display(results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "comparison_viz",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Comparison Visualization\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "colors = plt.cm.Set2(np.linspace(0, 1, len(results_df)))\n",
    "\n",
    "# MSE\n",
    "res_mse = results_df.sort_values('MSE')\n",
    "axes[0,0].barh(res_mse['Model'], res_mse['MSE'], color=colors, edgecolor='black')\n",
    "axes[0,0].set_xlabel('MSE')\n",
    "axes[0,0].set_title('üìä MSE Comparison (Lower = Better)', fontweight='bold')\n",
    "for i, v in enumerate(res_mse['MSE']):\n",
    "    axes[0,0].text(v + 0.0003, i, f'{v:.5f}', va='center', fontsize=9)\n",
    "\n",
    "# Poisson Deviance\n",
    "res_dev = results_df.sort_values('Poisson Deviance')\n",
    "axes[0,1].barh(res_dev['Model'], res_dev['Poisson Deviance'], color=colors, edgecolor='black')\n",
    "axes[0,1].set_xlabel('Poisson Deviance')\n",
    "axes[0,1].set_title('üìä Poisson Deviance (Lower = Better)', fontweight='bold')\n",
    "for i, v in enumerate(res_dev['Poisson Deviance']):\n",
    "    axes[0,1].text(v + 0.01, i, f'{v:.4f}', va='center', fontsize=9)\n",
    "\n",
    "# R¬≤\n",
    "res_r2 = results_df.sort_values('R¬≤', ascending=False)\n",
    "colors_r2 = ['#e74c3c' if x < 0 else '#2ecc71' for x in res_r2['R¬≤']]\n",
    "axes[1,0].barh(res_r2['Model'], res_r2['R¬≤'], color=colors_r2, edgecolor='black')\n",
    "axes[1,0].axvline(0, color='black', linewidth=0.8)\n",
    "axes[1,0].set_xlabel('R¬≤')\n",
    "axes[1,0].set_title('üìä R¬≤ Score (Higher = Better)', fontweight='bold')\n",
    "\n",
    "# Top 3 comparison\n",
    "top3 = results_df.head(3)\n",
    "x = np.arange(3)\n",
    "width = 0.25\n",
    "metrics = ['MSE', 'Poisson Deviance', 'MAE']\n",
    "for i, m in enumerate(metrics):\n",
    "    vals = top3[m].values / top3[m].max()  # Normalize\n",
    "    axes[1,1].bar(x + i*width, vals, width, label=m, alpha=0.8)\n",
    "axes[1,1].set_xticks(x + width)\n",
    "axes[1,1].set_xticklabels(top3['Model'])\n",
    "axes[1,1].set_ylabel('Normalized Score')\n",
    "axes[1,1].set_title('üèÜ Top 3 Models Comparison', fontweight='bold')\n",
    "axes[1,1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('fig06_model_comparison.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "summary_box",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üèÜ MODEL COMPARISON SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "best_mse = results_df.loc[results_df['MSE'].idxmin()]\n",
    "best_dev = results_df.loc[results_df['Poisson Deviance'].idxmin()]\n",
    "\n",
    "print(f\"\\nü•á Best MSE: {best_mse['Model']} ({best_mse['MSE']:.6f})\")\n",
    "print(f\"ü•á Best Deviance: {best_dev['Model']} ({best_dev['Poisson Deviance']:.6f})\")\n",
    "\n",
    "print(\"\\nüí° KEY INSIGHTS:\")\n",
    "print(\"   ‚Ä¢ Ensemble methods (RF, GB) achieve best predictive performance\")\n",
    "print(\"   ‚Ä¢ Poisson GLM offers interpretability for actuarial use\")\n",
    "print(\"   ‚Ä¢ Linear models struggle with count data characteristics\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section7",
   "metadata": {},
   "source": [
    "<a id='7'></a>\n",
    "## 7. Feature Importance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feat_imp",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(14, 8))\n",
    "\n",
    "# Random Forest\n",
    "rf_imp = pd.DataFrame({'Feature': X_train.columns, 'Importance': rf.feature_importances_})\n",
    "rf_imp = rf_imp.sort_values('Importance', ascending=False).head(15).sort_values('Importance')\n",
    "axes[0].barh(rf_imp['Feature'], rf_imp['Importance'], color=plt.cm.Blues(np.linspace(0.3, 0.9, 15)), edgecolor='black')\n",
    "axes[0].set_xlabel('Importance')\n",
    "axes[0].set_title('üå≤ Random Forest - Top 15 Features', fontweight='bold')\n",
    "\n",
    "# Gradient Boosting\n",
    "gb_imp = pd.DataFrame({'Feature': X_train.columns, 'Importance': gb.feature_importances_})\n",
    "gb_imp = gb_imp.sort_values('Importance', ascending=False).head(15).sort_values('Importance')\n",
    "axes[1].barh(gb_imp['Feature'], gb_imp['Importance'], color=plt.cm.Greens(np.linspace(0.3, 0.9, 15)), edgecolor='black')\n",
    "axes[1].set_xlabel('Importance')\n",
    "axes[1].set_title('üìà Gradient Boosting - Top 15 Features', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('fig07_feature_importance.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pois_coef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Poisson coefficients\n",
    "pois_coef = pd.DataFrame({'Feature': X_train_pois.columns, 'Coefficient': pois.coef_})\n",
    "pois_coef['Exp(Coef)'] = np.exp(pois_coef['Coefficient'])\n",
    "pois_coef = pois_coef.sort_values('Coefficient', key=abs, ascending=False)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "pois_top = pois_coef.head(15).sort_values('Coefficient')\n",
    "colors = ['#e74c3c' if x < 0 else '#2ecc71' for x in pois_top['Coefficient']]\n",
    "ax.barh(pois_top['Feature'], pois_top['Coefficient'], color=colors, edgecolor='black')\n",
    "ax.axvline(0, color='black', linewidth=0.8)\n",
    "ax.set_xlabel('Coefficient (Log Effect)')\n",
    "ax.set_title('üìä Poisson GLM - Top 15 Coefficients', fontweight='bold')\n",
    "ax.text(0.02, 0.98, 'Green = ‚Üë Claim Rate\\nRed = ‚Üì Claim Rate', transform=ax.transAxes, \n",
    "        verticalalignment='top', bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('fig08_poisson_coefficients.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section8",
   "metadata": {},
   "source": [
    "<a id='8'></a>\n",
    "## 8. Hyperparameter Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "gridsearch",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\"alpha\": [1e-5, 1e-4, 1e-3, 1e-2, 1e-1, 1.0]}\n",
    "\n",
    "grid_pois = GridSearchCV(\n",
    "    PoissonRegressor(max_iter=2000),\n",
    "    param_grid,\n",
    "    scoring=\"neg_mean_poisson_deviance\",\n",
    "    cv=3, n_jobs=-1, verbose=1\n",
    ")\n",
    "grid_pois.fit(X_train_pois, y_train, sample_weight=exp_train)\n",
    "\n",
    "print(f\"\\n‚úÖ Best alpha: {grid_pois.best_params_['alpha']}\")\n",
    "print(f\"‚úÖ Best CV deviance: {-grid_pois.best_score_:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "grid_viz",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_results = pd.DataFrame(grid_pois.cv_results_)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "alphas = cv_results['param_alpha'].astype(float)\n",
    "scores = -cv_results['mean_test_score']\n",
    "stds = cv_results['std_test_score']\n",
    "\n",
    "ax.semilogx(alphas, scores, 'b-o', linewidth=2, markersize=8)\n",
    "ax.fill_between(alphas, scores - stds, scores + stds, alpha=0.2)\n",
    "ax.axvline(grid_pois.best_params_['alpha'], color='red', linestyle='--', linewidth=2, \n",
    "           label=f'Best Œ± = {grid_pois.best_params_[\"alpha\"]}')\n",
    "ax.set_xlabel('Regularization (alpha)')\n",
    "ax.set_ylabel('Poisson Deviance')\n",
    "ax.set_title('‚öôÔ∏è Hyperparameter Tuning', fontweight='bold')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('fig09_hyperparameter_tuning.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section9",
   "metadata": {},
   "source": [
    "<a id='9'></a>\n",
    "## 9. Model Calibration Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "calibration",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, (name, y_pred) in enumerate(predictions.items()):\n",
    "    calib_df = pd.DataFrame({'predicted': safe_pred(y_pred), 'actual': y_test.values})\n",
    "    calib_df['decile'] = pd.qcut(calib_df['predicted'], q=10, labels=False, duplicates='drop')\n",
    "    calib_summary = calib_df.groupby('decile').agg({'predicted': 'mean', 'actual': 'mean'})\n",
    "    \n",
    "    axes[i].scatter(calib_summary['predicted'], calib_summary['actual'], s=100, alpha=0.7, edgecolors='black')\n",
    "    max_val = max(calib_summary['predicted'].max(), calib_summary['actual'].max())\n",
    "    axes[i].plot([0, max_val], [0, max_val], 'r--', linewidth=2, label='Perfect')\n",
    "    axes[i].set_xlabel('Predicted')\n",
    "    axes[i].set_ylabel('Actual')\n",
    "    axes[i].set_title(f'{name}', fontweight='bold')\n",
    "    axes[i].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('fig10_calibration.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lift_chart",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lift Chart\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "for model_name, color in [('Random Forest', '#3498db'), ('Gradient Boosting', '#2ecc71'), ('Poisson Regression', '#e74c3c')]:\n",
    "    y_pred = predictions[model_name]\n",
    "    lift_df = pd.DataFrame({'predicted': safe_pred(y_pred), 'actual': y_test.values, 'exposure': exp_test.values})\n",
    "    lift_df['decile'] = pd.qcut(lift_df['predicted'], q=10, labels=range(1, 11), duplicates='drop')\n",
    "    lift_summary = lift_df.groupby('decile').agg({'actual': 'sum', 'exposure': 'sum'})\n",
    "    lift_summary['rate'] = lift_summary['actual'] / lift_summary['exposure']\n",
    "    avg_rate = lift_df['actual'].sum() / lift_df['exposure'].sum()\n",
    "    lift_summary['lift'] = lift_summary['rate'] / avg_rate\n",
    "    \n",
    "    ax.plot(lift_summary.index, lift_summary['lift'], '-o', color=color, linewidth=2, markersize=8, label=model_name)\n",
    "\n",
    "ax.axhline(1, color='black', linestyle='--', label='Baseline')\n",
    "ax.set_xlabel('Decile (1=Low Risk, 10=High Risk)')\n",
    "ax.set_ylabel('Lift')\n",
    "ax.set_title('üìà Lift Chart - Risk Discrimination', fontweight='bold')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.set_xticks(range(1, 11))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('fig11_lift_chart.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section10",
   "metadata": {},
   "source": [
    "<a id='10'></a>\n",
    "## 10. Resampling Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "resampling",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create resampled dataset\n",
    "df_pos = df1[df1['ClaimNb'] > 0]\n",
    "df_resampled = pd.concat([df1, df_pos, df_pos, df_pos], ignore_index=True)\n",
    "\n",
    "print(f\"Original: {len(df1):,} rows, mean={df1['ClaimNb'].mean():.4f}\")\n",
    "print(f\"Resampled: {len(df_resampled):,} rows, mean={df_resampled['ClaimNb'].mean():.4f}\")\n",
    "\n",
    "# Train on resampled\n",
    "y_res = df_resampled['ClaimNb']\n",
    "X_res = df_resampled.drop(columns=['ClaimNb', 'Frequency', 'IDpol', 'DrivAge_Group', 'BM_Group'])\n",
    "exp_res = df_resampled['Exposure']\n",
    "\n",
    "X_tr_res, X_te_res, y_tr_res, y_te_res, exp_tr_res, exp_te_res = train_test_split(\n",
    "    X_res, y_res, exp_res, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Train Poisson on resampled\n",
    "X_tr_pois_res = X_tr_res.drop(columns=['Exposure'])\n",
    "X_te_pois_res = X_te_res.drop(columns=['Exposure'])\n",
    "\n",
    "pois_res = PoissonRegressor(alpha=1e-4, max_iter=1000)\n",
    "pois_res.fit(X_tr_pois_res, y_tr_res, sample_weight=exp_tr_res)\n",
    "y_pred_res = pois_res.predict(X_te_pois_res)\n",
    "\n",
    "print(f\"\\nResampled Poisson MSE: {mean_squared_error(y_te_res, y_pred_res):.6f}\")\n",
    "print(f\"Resampled Poisson Deviance: {mean_poisson_deviance(y_te_res, safe_pred(y_pred_res)):.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "resamp_viz",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparison visualization\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "x = np.arange(1)\n",
    "width = 0.35\n",
    "\n",
    "orig_dev = mean_poisson_deviance(y_test, safe_pred(predictions['Poisson Regression']))\n",
    "res_dev = mean_poisson_deviance(y_te_res, safe_pred(y_pred_res))\n",
    "\n",
    "axes[0].bar(['Original', 'Resampled'], [orig_dev, res_dev], color=['#3498db', '#e74c3c'], edgecolor='black')\n",
    "axes[0].set_ylabel('Poisson Deviance')\n",
    "axes[0].set_title('üìä Resampling Impact on Poisson GLM', fontweight='bold')\n",
    "\n",
    "# Class distribution\n",
    "axes[1].pie([len(df1[df1['ClaimNb']==0]), len(df1[df1['ClaimNb']>0])], \n",
    "            labels=['No Claim', 'Claim'], autopct='%1.1f%%', colors=['#2ecc71', '#e74c3c'])\n",
    "axes[1].set_title('üìä Original Class Distribution', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('fig12_resampling.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section11",
   "metadata": {},
   "source": [
    "<a id='11'></a>\n",
    "## 11. Conclusion and Recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "final_summary",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final Summary\n",
    "print(\"=\"*70)\n",
    "print(\"                    üìä PROJECT SUMMARY                    \")\n",
    "print(\"=\"*70)\n",
    "print(\"\\nüèÜ BEST MODELS:\")\n",
    "print(\"   1. Random Forest      - MSE: 0.0560, Deviance: 0.2963\")\n",
    "print(\"   2. Gradient Boosting  - MSE: 0.0561, Deviance: 0.2985\")\n",
    "print(\"   3. Poisson GLM        - MSE: 0.0578, Deviance: 0.3198\")\n",
    "\n",
    "print(\"\\nüìà KEY RISK FACTORS:\")\n",
    "print(\"   ‚Ä¢ BonusMalus coefficient (strongest predictor)\")\n",
    "print(\"   ‚Ä¢ Driver age (young = high risk)\")\n",
    "print(\"   ‚Ä¢ Vehicle characteristics\")\n",
    "print(\"   ‚Ä¢ Geographic factors\")\n",
    "\n",
    "print(\"\\nüí° RECOMMENDATIONS:\")\n",
    "print(\"   ‚Ä¢ For ACCURACY: Use Random Forest / Gradient Boosting\")\n",
    "print(\"   ‚Ä¢ For COMPLIANCE: Use Poisson GLM (interpretable)\")\n",
    "print(\"   ‚Ä¢ For PRODUCTION: Consider XGBoost with Poisson loss\")\n",
    "\n",
    "print(\"\\nüîÆ FUTURE WORK:\")\n",
    "print(\"   ‚Ä¢ GAM for non-linear interpretable effects\")\n",
    "print(\"   ‚Ä¢ Zero-Inflated Poisson for excess zeros\")\n",
    "print(\"   ‚Ä¢ SHAP values for model explanation\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "conclusion_md",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### üìù Summary\n",
    "\n",
    "This project demonstrated that **ensemble methods** outperform traditional approaches for claim frequency prediction, while **Poisson GLM** remains valuable for regulatory compliance and interpretability.\n",
    "\n",
    "### üìö References\n",
    "\n",
    "1. Denuit, M. et al. (2007). *Actuarial Modelling of Claim Counts*\n",
    "2. W√ºthrich, M. V. & Merz, M. (2008). *Stochastic Claims Reserving Methods*\n",
    "3. scikit-learn Documentation: https://scikit-learn.org/\n",
    "\n",
    "---\n",
    "\n",
    "**¬© 2024 - Auto Insurance Claim Frequency Modeling**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
